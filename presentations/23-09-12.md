# Meeting met Prof. Develder

Verwachtingen:
```
Vóór 9 september zal ik geen tijd kunnen vrijmaken je thesis (in huidige versie) te lezen.
Het lijkt me efficiënter dat je de call/meeting in september grondig voorbereidt,
en een presentatie voorziet waarin je duidelijk
(1) context;
(2) probleemstelling, ie. wetenschappelijke onderzoeksvragen;
(3) aanpak/methodologie om 2 te beantwoorden;
(4) belangrijkste resultaten (i.e. experimentele resultaten, kwalitatief/quantitatief) samenvat.
```

# Context

MBSE

openCAESAR

Ontologies

Viewpoints --> report & views

ChatGPT example

# Probleemstelling

Hoe onderzoeksvragen introduceren zonder aanpak eerst uitgelegd to hebben?

## R1:
How does the performance of the proposed domain expert compare to other approaches across various data availability
scenarios?

## R2:
Does the domain expert effectively utilize the retrieved information with which it is soft-prompted,
and what factors influence the effectiveness of these prompts?

## R3:
What is the impact of using synthetic data, as opposed to ground truth data,
on domain adaptation performance during the second learning  phase?

# Methodologie



# Resultaten

## Retriever Parameters

## Synthetic Dataset Quality

## Text-to-Prelinked-SQUALL

Gerelateerd aan R2.

## Generalization Capabilities

Gerelateerd aan R3.

## Text to SPARQL

Gerelateerd aan R1.




# Notes

% Paradigm
To address this flexibility gap, a no-code interactive paradigm of model reporting for \gls{mbse} is proposed.
The proof-of-concept that is put forward empowers the stakeholders by meeting the demand for flexibility
while staying within the boundaries of the technical skills of the potential users.
It allows stakeholders to more quickly access information and more easily document complex systems.
The paradigm is based around the notebook environment from within which a stakeholder can interact with an agent
-- through natural language interaction 
-- who answers question about the model with explanations, tabular results and visualizations.
Stakeholders can form a report in this environment in an easy and quick way without help from a domain expert or 
technologist.

% Approach
The agent is tasked with transforming a question into an appropriate response.
While \glsentryfull{qa} is a well-established area within \gls{nlp}, it traditionally focuses on generating \glspl{nla}.
However, model reporting extends beyond text-based answers;
visualizations and access to tabular data are essential components.
Therefore, the agent must not only generate natural language responses but also explicitly produce queries that
facilitate these additional tasks.
To achieve this, the agent leverages advancements in \gls{nlp},
particularly by harnessing the capabilities of \gls{sota} \glspl{llm},
which are increasingly being used to address complex, multi-faceted challenges in data-driven tasks.

% Problem
The key problem with explicit query construction for model reporting is the lack of data.
The ontologies used in \gls{mbse} are often private and highly specialized.
Lack of data is of course an extremely common issue, but the above two realities make it especially troublesome.
No high-quality, sizeable and domain relevant labeled datasets can be assumed to exist.
This challenge is referred to as the ``data scarcity scenario'' throughout this work.
The primary focus of the remainder of this study is to address and overcome this issue of data scarcity.

% Illustrative example
To illustrate the issue, consider the following example involving the \gls{orkg},
which includes papers, their contributors, topics, and more.
Suppose a non-technical stakeholder, without domain expertise, has a burning question.
Their first impulse might be to use ChatGPT.
For instance, they might ask:
``Provide a list of papers that have utilized the Depth DDPPO model and include the links to their code''
\cite{yaserjaradehOrkgSciQA}.
Despite providing ChatGPT with relevant \gls{rdf} data or even the exact necessary triples,
they would likely find that ChatGPT is unable to generate a sensible query.
This issue arises even though ChatGPT is knowledgeable about \gls{sparql} and has access to domain-specific information.
The challenge lies in the relative obscurity of \gls{orkg} compared to more widely-known \glspl{kg}
and the high complexity of the query itself.
See \cref{lst:sciqa_query} and \cref{lst:sciqa_query_chatgpt} for the expected and generated query, resp.
Evidently, this is a rather naive approach and much more intricate solutions can be built using \glspl{pllm}.
Nevertheless, it does get to the heart of the problem:
generating complex \gls{sparql} queries for obscure domains is not trivial.
\TODO{I forget why a paper \cite{dialloComprehensiveEvaluationNeural2024} was relevant here.}

\input{src/listings/key-problem-examples}

% How is the problem addressed
Data scarcity is addressed through a three-pronged approach.
First, the generation target language is shifted from \gls{sparql} to \gls{squall}
\cite{ferreSQUALLExpressivenessSPARQL2014}, see \cref{lst:squall_example} for an example.
\gls{squall} has been shown to reduce data requirements for \glspl{llm} because it is closer to \gls{nl}
\cite{lehmannLanguageModelsControlled2023}.
Second, transfer learning is leveraged through a \gls{rag} paradigm, involving:
\begin{itemize}

	\item \textbf{Initial Fine-Tuning:}
		Fine-tuning of an \gls{llm} in a data-rich environment, followed by freezing it,
		as the first phase of a sequential training strategy.

	\item \textbf{Integration in \gls{rag} Model:}
		Using the fine-tuned \gls{llm} as a generator within a \gls{rag} framework.

	\item \textbf{Retrieval and Augmentation:}
		Using a static retriever and post-retrieval processing for non-parametric information, and integrating it through
		a trainable augmentation method incorporated in the intermediate layers of the frozen \gls{llm} (soft prompting).
	
	\item \textbf{Domain Adaptation:}
		Conduct a second phase of training on the target domain, leveraging the frozen \gls{llm} for its retained
		capabilities while adapting through augmentation.

\end{itemize}
The resulting model is dubbed the ``domain expert''.
Finally, to simulate a realistic scenario with some available data,
a synthetic data is generated from the limited dataset to test the effectiveness of the proposed approach.
In conclusion, data scarcity is tackled through a \gls{cnl}, \gls{rag}-powered transfer learning
and synthetic data generation.

\input{src/listings/squall-example}

% Research questions
From the core problem, the following research questions are formulated:
\TODO{More specific?}
\textbf{R1:}
How does the performance of the proposed domain expert compare to other approaches
across various data availability scenarios?
\textbf{R2:}
Does the domain expert effectively utilize the retrieved information with which it is soft-prompted,
and what factors influence the effectiveness of these prompts?
\textbf{R3:}
What is the impact of using synthetic data, as opposed to ground truth data,
on domain adaptation performance during the second learning phase?

The contributions of this thesis include the introduction of
a novel paradigm of interactive model reporting for the \gls{mbse} community that allows a previously
unattainable level of flexibility with the open-sourced accompanying proof-of-concept implementation.
Furthermore, an approach to the text-to-\gls{sparql} task is presented that is specifically designed for realistic data
scarce scenarios using a pipeline that incorporates the proposed domain expert.
Additionally, an evaluation and comparison of the presented approach with similar methods on a representative \gls{kg}
is included.
Lastly, the domain expert is provided on HuggingFace\footnote{\TODO{Upload to HugggingFace.}}.

% Recap
This work advances the \gls{sota} in model reporting for \gls{mbse} by its proposed paradigm shift,
enabled through an approach addressing the text-to-\gls{sparql} task under conditions of data scarcity.
