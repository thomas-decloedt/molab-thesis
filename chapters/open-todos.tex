\section{Synthetic Dataset Generation}
\TODO{Investigate quality synthetic dataset with and without generated squall of base model.}

Currently, the approach to generating the synthetic dataset is rather limited.
For future exploration, looking into extending the prompt's examples to be much more comprehensive and varied,
in terms of \gls{kg} and \gls{squall} coverage, is worthwhile.
This means providing examples that for one touch on more entities and properties,
and on the other hand contain many more query structures.
Furthermore, the addition of the input question's textualized subgraph
(the \gls{pcst} algorithm does not require training, so this is possible)
or an initial \gls{squall} query generated by the base model
to the prompt could be interesting to investigate.

\subsection{Generate Both Questions and Answers Simulataneously From Passages}
\label{s:knowledge_graph_and_benchmark}

End-to-End Synthetic Data Generation for Domain Adaptation of Question Answering Systems
\cite{shakeriEndtoEndSyntheticData2020}.

\subsection{Data Eliciation to Generate Questions}

Strategy to generate questions in the first place.
Data elicitation to generate questions see \cite{chengBuildingNeuralSemantic2018}.
Semantic parser from data elicitation method see \cite{chengBuildingNeuralSemantic2018}.
Soft graph prompt with parsed tree of question using above parser.
See table 6, domain-specific lexicon: rdfs:labels.
See table 7, canonical representations described by templates mapped to natural language with PLLM.
Maybe \gls{squall} can be generated straight from canonical representations too.

\subsection{Synthetic Data from SCFG Rules} 

From \cite{liSemanticParsingLimited2023}:

In this chapter, we develop a semantic parser to improve human-system interaction in the maritime domain when no
parallel data are available and only structured data stored  in the database exist.
By developing a compact set of SCFG rules given the database  data and multiple automatic paraphrasing techniques,
it is possible to generate massive  amounts of synthetic data at a very low cost.
The parser trained on such data can also  achieve a promising result on the manually annotated test set.
This demonstrates that 58 our approach could be useful in many real-world situations
where parallel training data is scarce or unavailable.


\subsection{Mixed Source Domain Data to Improve SQUALL Expert}

Domain adaptation could work with a mixed source domain \cite{mansourDomainAdaptationMultiple2008}.

\subsection{Prompting}

Generate synthetic data in the fashion that similar work \cite{lehmannLargeLanguageModels2024} prompts \glspl{llm}.

\section{Semantic Parsing}

Why not extend retrieval and soft prompting?
(Besides passage retrieval and input prompting.)
There's possibly more graph information to be generated.

Soft graph prompting of base model where graph token are parse trees from a constituency or dependency parser?

See \cite{ferreSQUALLExpressivenessSPARQL2014}:
Using SQUALL as an intermediate representation.
If full natural language is to be accepted as input, a solution is to use mature NLP tools (e.g., Stanford NLP parser)
to parse a spontaneous sentence, and then translate the resulting dependency graph to SQUALL,
which is arguably much closer to NL than SPARQL is.

Can domain adaptation of the parser be achieved through \cite{chengBuildingNeuralSemantic2018}?

Idea:
\begin{enumerate}
	\item Lean a constituency or dependency \gls{squall} parser in the source domain: \gls{squall} parser.
	\item Domain adaptation of semantic parser to target domain.
	\item Parse a natural language question: parse tree.
	\item Encode tree: tree token.
	\item Project tree: aligned tree token.
	\item Concatenate to aligned graph token and text tokens.
\end{enumerate}


\section{Formalization}

Try to find theoretical bounds on \gls{pgp} and \gls{gp} isomorphism, correct mapping, etc. see graph theory book.

\section{Linking}

ontology-based subgraph querying

See \Cref{zhengSemanticSPARQLSimilarity2016}, and \Cref{wuOntologybasedSubgraphQuerying2013}

\section{Evaluation}

\paragraph{Uninferrable Info}

Concern of Prof. Elaasar: how to infer triples (i.e., generate \gls{sparql}) not deducible from the question?

\paragraph{User Study}

To evaluate the usefulness of the paradigm, a user study could be interesting,
similar to \cite{wangSlide4NCreatingPresentation2023}.

\section{To Classify}


\paragraph{Enhanced AI Agent}
The current rudimentary proof-of-concept implementation of the ``\gls{ai} agent'' could be developed into a
fully fledged, sophisticated agent.
This advanced version might for example be capable of processing feedback, engaging in coherent conversations,
adhering to specified answer styles, and providing advanced visualizations.
It could also possess the ability to accurately recognize user intent â€” whether to visualize data,
provide explanations, or list information.
Furthermore, supporting additional operations and transformations on the data,
such as generating statistical analyses from query results.

\paragraph{OML Ontologies}
Evaluate the approach on a specific non-trivial \gls{oml} ontology.

\paragraph{Prompting Instead of Fine-Tuning for Text-to-Prelinked-SQUALL Task}
An \gls{llm} was fine-tuned for the specific task of generation \gls{squall} given a question.
Instead, a powerful \gls{pllm} such as ChatGPT might be used to do the same, 
enriching the prompt with the textualized subgraph from the retriever or ontology passages, for example,
if such an alternative retrieval approach is used.
Comparing both approaches in terms of execution time or accuracy
(hallucinations might be a problem for the \gls{pllm}) could be interesting.

\paragraph{Ablation Study Large Language Model}
Explore different base \glspl{lm} besides Llama2.

\paragraph{Ablation Study Graph Encoders}
According to experiments \cite{heGRetrieverRetrievalAugmentedGeneration2024},
the choice of specific graph encoder can have a non-trivial impact on performance.
In practice, experimentation with various models would be important
such that the model is selected based on the specific \gls{kg}.

\paragraph{Alternative Knowledge Graph and Dataset}
\TODO{Reference limited data variety due to use rdfs:labels instead of nnqt, see section methodology, dataset, retr}
\gls{sparql} queries in SciQA dataset are not varied enough, might be worth it to look into DBLP-QuAD
for the DBLP \gls{kg}.\footnote{
	DBLP-QuAD dataset \url{https://huggingface.co/datasets/awalesushil/DBLP-QuAD?row=0}.
}\footnote{
	DBLP homepage \url{https://blog.dblp.org}.
}

\paragraph{Generalization of Retrieval Augmentation}
Currently, the retrieval augmentation is \gls{kg} dependent.
This shortcoming has been addressed \cite{vermaLearningUniversalGraph2019} in the broader context of graph learning
tasks, where they propose a novel graph embedding network in an attempt to make the graph embeddings task-independent.
As yet, no such pre-trained universal graph embedder is published,
but it would be intriguing to investigate the possibilities of such a model in the future for this work's use case
and data-scarce assumption.

\paragraph{Trained Retriever}
An ongoing challenge in working with \glspl{kg} is their dynamic nature,
as they frequently evolve with the addition and removal of vertices and edges, as well as updates to existing labels.
This poses significant difficulties for approaches that rely on embeddings of the \gls{kg},
particularly when a model must be continuously retrained to reflect these changes.
Lifelong \gls{kg} embedding learning has emerged as a research area addressing these challenges,
including issues like catastrophic forgetting.
The proposed approach, which leverages a \gls{plm} to generate embeddings,
simplifies the task of keeping the index up-to-date which reduces to merely tracking changes to the \gls{kg}
and updating the index accordingly.
For future work, a promising direction involves exploring the training of a retriever under data-scarce conditions.
Previous work demonstrates \cite{sachanQuestionsAreAll2023} that retrievers can be effectively trained
using only questions (potentially generated questions), a similar strategy can be envisioned.
By relying solely on a set of questions and generating labels via a \gls{pllm},
one might create a synthetic dataset to not only train the \gls{gnn} and projector,
but also the encoder.
--- or dual encoder consisting of question and text encoder
(for the vertices and edge labels in this work, although ontology passages are possible too)
in the case of \cite{sachanQuestionsAreAll2023}.
This approach could further enhance the robustness of the system, and it opens up avenues for optimizing the retriever
training process to be more efficient and adaptable to evolving \glspl{kg}.

\paragraph{Graph Preprocessing}
Preprocessing of the \gls{kg} in the retriever to reduce its size before passing it to the \gls{pcst} algo.
Using previous work for example \cite{jiangUniKGQAUnifiedRetrieval2023, heImprovingMultihopKnowledge2021} following
\cite{luoReasoningGraphsFaithful2024}.

\paragraph{Text Based Retrieval}
Compare graph vs text based retrieval.
openCAESAR's starting point is the ontology (text) which is then turned into a \gls{kg} (graph),
using Apache Jena Fuseki, and stored in an \gls{rdf} triplestore.
Retrieval of paragraphs from the ontology could for example be evaluated against the approach presented here.

\paragraph{Subset Ontology/KG for Index}
Currently the index is constructed from the entire ontology/\gls{kg}, i.e., TBox and ABox.
Future experiments might restrict the index to TBox statements from the ontology or associated triples from the
\gls{rdf}-store.

\paragraph{Linking}
Linking between vertex or edge label and its respective placeholder is too simplistic.




% NEW EVALUATION?
Geo880 \cite{wangBuildingSemanticParser2015} nope: SQL not SPARQL
or ATIS 
% https://paperswithcode.com/sota/semantic-parsing-on-atis


% Contribution
\TODO{Cite the open challenge from \cite{elaasarOpenCAESARBalancingAgility2023}: Stakeholder Engagement}
\TODO{Transfer learning! One domain to another}

% General
\TODO{Make sure to user the semantics and structure of a query correctly, see \Cref{zhengSemanticSPARQLSimilarity2016}}
\TODO{Typo on cover sheet: openCAESAR}
\TODO{Thank word}
\TODO{Update abstract}
\TODO{Update intro}
\TODO{Update prelims}
\TODO{Add final experimental results}
\TODO{
	Write Conclusion, add graph showing reduction in data engineering and recap in terms of function composition:
%	s ---AG---> S ---MAP---> \hat{Q} ---LINK---> Q ---QUERY---> Results
%	|    ^
%	R    |     
%	| ---H
}
\TODO{Reread future work}
\TODO{Running example for the entire paper}

% Experimentation
\TODO{
	Domain adaptation performance of different RAG models and different domains
	\cite{siriwardhanaImprovingDomainAdaptation2023}.
}
\TODO{
	Check the maximum size of the soft prompt and make it sure it is not exceeded in the experimental setups.
}

% PCST
\TODO{How is the connected graph obtained? Why is it connected after PCST?}

% Data
\TODO{How are the placeholders in LC-QuAD 2.0 gotten?}

% Justifications
\TODO{Why use Llama? It's auto-encodings, why not Seq2Seq?}

% Figures & tables
\TODO{Example notebook}
\TODO{Clean up schematics}
\TODO{Add note to tables to say when percentages, reduces text}

% Formatting
\TODO{Don't italicize all i.e., e.g., s.t., etc.}
\TODO{Fix dashes}

% Bibliography
\TODO{Make sure KGQAn and G-Retriever are properly given credit!}
\TODO{Clean up use of urls}
\TODO{Check references}
\TODO{Update to github/code references when appropriate.}
\TODO{Add repo contributors of software on Github to Zotero}

% Notation
\TODO{Unify math notation for sets and definitions.}
\TODO{Replace all k with $k$.}
\TODO{Use and symbol or comma in sets}
\TODO{Replace lowercase \texttt{optional} etc. with upper-case.}

% Linking
\TODO{Give the probability of correctly linking.}
\TODO{Open question:
	Does linking benefit from a more faithful model, because it generates placeholders closer to actual 
	\texttt{rdfs:label}s, reducing downstream ambiguity and simplifying linking?
	Seems reasonable.
}

% Domain expert
\TODO{
	Train domain expert with subset of ground truth data, to see how much data we can get away with for halfway 
	decent results?
}

\TODO{Future work: error analysis of our approach, syntactic, semantic, entities/predicates, i.e., URIs, etc.} 

% Retriever
\TODO{
	Questions are all you need fine-tuning of DPR on ontology passages.
	Then freeze, use as is in RAG.
	Impact of not specialised to task (text-to-SQUALL)?
	Questions are all you need training on RAG?
}
% RAG
\TODO{Why not add an additional Llama adapter for specific domain/KG}
\TODO{
	Strategy to generate questions in the first place.
	Secondly, parser might supplant or support current soft graph prompting strategy.
	Data elicitation to generate questions see \cite{chengBuildingNeuralSemantic2018}.
	Semantic parser from data elicitation method see \cite{chengBuildingNeuralSemantic2018}.
	Soft graph prompt with parsed tree of question using above parser.
	See table 6, domain-specific lexicon: rdfs:labels.
	See table 7, canonical representations described by templates mapped to natural language with PLLM.
	See \cite{zouGraphBasedRDFData2017} for parsing examples.
}
% Linking
\TODO{
	Detecting matching placeholder and rdfs:label from constructed subgraph, ie, detecting matching spans,
	Siamese networks, Attention networks.
}

% Transfer learning
\TODO{Somewhat similar to what I do \cite{duffyStructuralTransferLearning2023}.}

\TODO{Upload models and datastes to hugging face}
\TODO{Look at query and semantic graph works.}

